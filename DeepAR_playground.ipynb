{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPdpJ3KhdQe9z6t7RE0wSb6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Olyco/deepar.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMQyOudRcMXe",
        "outputId": "ff1eabcc-4b51-47e2-acdd-f8e812e086ee"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deepar'...\n",
            "remote: Enumerating objects: 170, done.\u001b[K\n",
            "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 170 (delta 42), reused 54 (delta 29), pack-reused 89 (from 1)\u001b[K\n",
            "Receiving objects: 100% (170/170), 75.28 KiB | 2.89 MiB/s, done.\n",
            "Resolving deltas: 100% (79/79), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ФАН ТАЙМ"
      ],
      "metadata": {
        "id": "dMIK6-u9X2xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def gaussian_likelihood(sigma):\n",
        "    def gaussian_loss(y_true, y_pred):\n",
        "        return tf.reduce_mean(0.5*tf.log(sigma) + 0.5*tf.div(tf.square(y_true - y_pred), sigma)) + 1e-6 + 6\n",
        "    return gaussian_loss"
      ],
      "metadata": {
        "id": "4U0OZF5sOJZY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from keras.initializers import glorot_normal\n",
        "from keras.layers import Layer\n",
        "\n",
        "\n",
        "class GaussianLayer(Layer):\n",
        "    def __init__(self, output_dim, **kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        self.kernel_1, self.kernel_2, self.bias_1, self.bias_2 = [], [], [], []\n",
        "        super(GaussianLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        n_weight_rows = input_shape[2]\n",
        "        self.kernel_1 = self.add_weight(name='kernel_1',\n",
        "                                        shape=(n_weight_rows, self.output_dim),\n",
        "                                        initializer=glorot_normal(),\n",
        "                                        trainable=True)\n",
        "        self.kernel_2 = self.add_weight(name='kernel_2',\n",
        "                                        shape=(n_weight_rows, self.output_dim),\n",
        "                                        initializer=glorot_normal(),\n",
        "                                        trainable=True)\n",
        "        self.bias_1 = self.add_weight(name='bias_1',\n",
        "                                      shape=(self.output_dim,),\n",
        "                                      initializer=glorot_normal(),\n",
        "                                      trainable=True)\n",
        "        self.bias_2 = self.add_weight(name='bias_2',\n",
        "                                      shape=(self.output_dim,),\n",
        "                                      initializer=glorot_normal(),\n",
        "                                      trainable=True)\n",
        "        super(GaussianLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        output_mu = K.dot(x, self.kernel_1) + self.bias_1\n",
        "        output_sig = K.dot(x, self.kernel_2) + self.bias_2\n",
        "        output_sig_pos = K.log(1 + K.exp(output_sig)) + 1e-06\n",
        "        return [output_mu, output_sig_pos]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"\n",
        "        The assumption is the output ts is always one-dimensional\n",
        "        \"\"\"\n",
        "        return [(input_shape[0], self.output_dim), (input_shape[0], self.output_dim)]"
      ],
      "metadata": {
        "id": "jV9CnGjoYGIE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deepar.model import NNModel\n",
        "from deepar.model.layers import GaussianLayer\n",
        "from keras.layers import Input, Dense, Input\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM\n",
        "from keras import backend as K\n",
        "import logging\n",
        "from deepar.model.loss import gaussian_likelihood\n",
        "\n",
        "logger = logging.getLogger('deepar')\n",
        "\n",
        "\n",
        "class DeepAR(NNModel):\n",
        "    def __init__(self, ts_obj, steps_per_epoch=50, epochs=100, loss=gaussian_likelihood,\n",
        "                 optimizer='adam', with_custom_nn_structure=None):\n",
        "\n",
        "        self.ts_obj = ts_obj\n",
        "        self.inputs, self.z_sample = None, None\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "        self.epochs = epochs\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "        self.keras_model = None\n",
        "        if with_custom_nn_structure:\n",
        "            self.nn_structure = with_custom_nn_structure\n",
        "        else:\n",
        "            self.nn_structure = DeepAR.basic_structure\n",
        "        self._output_layer_name = 'main_output'\n",
        "        self.get_intermediate = None\n",
        "\n",
        "    @staticmethod\n",
        "    def basic_structure():\n",
        "        \"\"\"\n",
        "        This is the method that needs to be patched when changing NN structure\n",
        "        :return: inputs_shape (tuple), inputs (Tensor), [loc, scale] (a list of theta parameters\n",
        "        of the target likelihood)\n",
        "        \"\"\"\n",
        "        input_shape = (20, 1)\n",
        "        inputs = Input(shape=input_shape)\n",
        "        x = LSTM(4, return_sequences=True)(inputs)\n",
        "        x = Dense(3, activation='relu')(x)\n",
        "        loc, scale = GaussianLayer(1, name='main_output')(x)\n",
        "        return input_shape, inputs, [loc, scale]\n",
        "\n",
        "    def instantiate_and_fit(self, verbose=False):\n",
        "        input_shape, inputs, theta = self.nn_structure()\n",
        "        model = Model(inputs, theta[0])\n",
        "        model.compile(loss=self.loss(theta[1]), optimizer=self.optimizer)\n",
        "        model.fit_generator(ts_generator(self.ts_obj,\n",
        "                                         input_shape[0]),\n",
        "                            steps_per_epoch=self.steps_per_epoch,\n",
        "                            epochs=self.epochs)\n",
        "        if verbose:\n",
        "            logger.debug('Model was successfully trained')\n",
        "        self.keras_model = model\n",
        "        self.get_intermediate = K.function(inputs=[self.model.input],\n",
        "                                           outputs=self.model.get_layer(self._output_layer_name).output)\n",
        "\n",
        "    @property\n",
        "    def model(self):\n",
        "        return self.keras_model\n",
        "\n",
        "    def predict_theta_from_input(self, input_list):\n",
        "        \"\"\"\n",
        "        This function takes an input of size equal to the n_steps specified in 'Input' when building the\n",
        "        network\n",
        "        :param input_list:\n",
        "        :return: [[]], a list of list. E.g. when using Gaussian layer this returns a list of two list,\n",
        "        corresponding to [[mu_values], [sigma_values]]\n",
        "        \"\"\"\n",
        "        if not self.get_intermediate:\n",
        "            raise ValueError('TF model must be trained first!')\n",
        "\n",
        "        return self.get_intermediate(input_list)\n",
        "\n",
        "\n",
        "def ts_generator(ts_obj, n_steps):\n",
        "    \"\"\"\n",
        "    This is a util generator function for Keras\n",
        "    :param ts_obj: a Dataset child class object that implements the 'next_batch' method\n",
        "    :param n_steps: parameter that specifies the length of the net's input tensor\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    while 1:\n",
        "        batch = ts_obj.next_batch(1, n_steps)\n",
        "        yield batch[0], batch[1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "oWHFaQzPYY89",
        "outputId": "9b67a36e-c853-4592-9567-cae2a12157a1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'deepar'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-4782bed7ece8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# from deepar.model import NNModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deepar'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deepar.dataset import Dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class TimeSeries(Dataset):\n",
        "    def __init__(self, pandas_df, one_hot_root_list=None, grouping_variable='category', scaler=None):\n",
        "        super().__init__()\n",
        "        self.data = pandas_df\n",
        "        self.one_hot_root_list = one_hot_root_list\n",
        "        self.grouping_variable = grouping_variable\n",
        "        if self.data is None:\n",
        "            raise ValueError('Must provide a Pandas df to instantiate this class')\n",
        "        self.scaler = scaler\n",
        "\n",
        "    def _one_hot_padding(self, pandas_df, padding_df):\n",
        "        \"\"\"\n",
        "        Util padding function\n",
        "        :param padding_df:\n",
        "        :param one_hot_root_list:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        for one_hot_root in self.one_hot_root_list:\n",
        "            one_hot_columns = [i for i in pandas_df.columns   # select columns equal to 1\n",
        "                               if i.startswith(one_hot_root) and pandas_df[i].values[0] == 1]\n",
        "            for col in one_hot_columns:\n",
        "                padding_df[col] = 1\n",
        "        return padding_df\n",
        "\n",
        "    def _pad_ts(self, pandas_df, desired_len, padding_val=0):\n",
        "        \"\"\"\n",
        "        Add padding int to the time series\n",
        "        :param pandas_df:\n",
        "        :param desired_len: (int)\n",
        "        :param padding_val: (int)\n",
        "        :return: X (feature_space), y\n",
        "        \"\"\"\n",
        "        pad_length = desired_len - pandas_df.shape[0]\n",
        "        padding_df = pd.concat([pd.DataFrame({col: padding_val for col in pandas_df.columns},\n",
        "                                             index=[i for i in range(pad_length)])])\n",
        "\n",
        "        if self.one_hot_root_list:\n",
        "            padding_df = self._one_hot_padding(pandas_df, padding_df)\n",
        "\n",
        "        return pd.concat([padding_df, pandas_df]).reset_index(drop=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def _sample_ts(pandas_df, desired_len):\n",
        "        \"\"\"\n",
        "        :param pandas_df: input pandas df with 'target' columns e features\n",
        "        :param desired_len: desired sample length (number of rows)\n",
        "        :param padding_val: default is 0\n",
        "        :param initial_obs: how many observations to skip at the beginning\n",
        "        :return: a pandas df (sample)\n",
        "        \"\"\"\n",
        "        if pandas_df.shape[0] < desired_len:\n",
        "            raise ValueError('Desired sample length is greater than df row len')\n",
        "        if pandas_df.shape[0] == desired_len:\n",
        "            return pandas_df\n",
        "\n",
        "        start_index = np.random.choice([i for i in range(0, pandas_df.shape[0] - desired_len + 1)])\n",
        "        return pandas_df.iloc[start_index: start_index+desired_len, ]\n",
        "\n",
        "    def next_batch(self, batch_size, n_steps,\n",
        "                   target_var='target', verbose=False,\n",
        "                   padding_value=0):\n",
        "        \"\"\"\n",
        "        :param batch_size: how many time series to be sampled in this batch (int)\n",
        "        :param n_steps: how many RNN cells (int)\n",
        "        :param target_var: (str)\n",
        "        :param verbose: (boolean)\n",
        "        :param padding_value: (float)\n",
        "        :return: X (feature space), y\n",
        "        \"\"\"\n",
        "\n",
        "        # Select n_batch time series\n",
        "        groups_list = self.data[self.grouping_variable].unique()\n",
        "        np.random.shuffle(groups_list)\n",
        "        selected_groups = groups_list[:batch_size]\n",
        "        input_data = self.data[self.data[self.grouping_variable].isin(set(selected_groups))]\n",
        "\n",
        "        # Initial padding for each selected time series to reach n_steps\n",
        "        sampled = []\n",
        "        for cat, cat_data in input_data.groupby(self.grouping_variable):\n",
        "                if cat_data.shape[0] < n_steps:\n",
        "                    sampled_cat_data = self._pad_ts(pandas_df=cat_data,\n",
        "                                                    desired_len=n_steps,\n",
        "                                                    padding_val=padding_value)\n",
        "                else:\n",
        "                    sampled_cat_data = self._sample_ts(pandas_df=cat_data,\n",
        "                                                       desired_len=n_steps)\n",
        "                sampled.append(sampled_cat_data)\n",
        "        rnn_output = pd.concat(sampled).drop(columns=self.grouping_variable).reset_index(drop=True)\n",
        "\n",
        "        if self.scaler:\n",
        "            batch_scaler = self.scaler()\n",
        "            n_rows = rnn_output.shape[0]\n",
        "            # Scaling must be extended to handle multivariate time series!\n",
        "            rnn_output['feature_1'] = rnn_output.feature_1.astype('float')\n",
        "            rnn_output[target_var] = rnn_output[target_var].astype('float')\n",
        "\n",
        "            rnn_output['feature_1'] = batch_scaler.fit_transform(rnn_output.feature_1.values.reshape(n_rows, 1)).reshape(n_rows)\n",
        "            rnn_output[target_var] = batch_scaler.fit_transform(rnn_output[target_var].values.reshape(n_rows, 1)).reshape(n_rows)\n",
        "\n",
        "        return rnn_output.drop(target_var, 1).as_matrix().reshape(batch_size, n_steps, -1), \\\n",
        "               rnn_output[target_var].as_matrix().reshape(batch_size, n_steps, 1)"
      ],
      "metadata": {
        "id": "0lhsTbm7Yeft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "air = pd.read_csv(\"AirPassengers.csv\")['#Passengers'].values\n",
        "source_df = pd.DataFrame({'feature_1': air[:-1], 'target': air[1:]})\n",
        "source_df['category'] = ['1' for i in range(source_df.shape[0])]"
      ],
      "metadata": {
        "id": "9ciKiZv_ZRW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deepar.dataset.time_series import TimeSeries\n",
        "from deepar.model.lstm import DeepAR\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "ts = TimeSeries(source_df, scaler=MinMaxScaler)\n",
        "dp_model = DeepAR(ts, epochs=100)\n",
        "dp_model.instantiate_and_fit()"
      ],
      "metadata": {
        "id": "iQ5ElyppZRcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from numpy.random import normal\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "batch = ts.next_batch(1, 20)\n",
        "\n",
        "def get_sample_prediction(sample, prediction_fn):\n",
        "    sample = np.array(sample).reshape(1, 20, 1)\n",
        "    output = prediction_fn([sample])\n",
        "    samples = []\n",
        "    for mu,sigma in zip(output[0].reshape(20), output[1].reshape(20)):\n",
        "        samples.append(normal(loc=mu, scale=np.sqrt(sigma), size=1)[0])\n",
        "    return np.array(samples)\n",
        "\n",
        "ress = []\n",
        "for i in tqdm.tqdm(range(300)):\n",
        "    pred = get_sample_prediction(batch[0], dp_model.predict_theta_from_input)\n",
        "    ress.append(pred)\n",
        "\n",
        "def plot_uncertainty(ress, ground_truth, n_steps=20, figsize=(9, 6),\n",
        "                     prediction_dots=True, title='Prediction on training set'):\n",
        "\n",
        "    res_df = pd.DataFrame(ress).T\n",
        "    tot_res = res_df\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.plot(ground_truth.reshape(n_steps), linewidth=6, label='Original data')\n",
        "    tot_res['mu'] = tot_res.apply(lambda x: np.mean(x), axis=1)\n",
        "    tot_res['upper'] = tot_res.apply(lambda x: np.mean(x) + np.std(x), axis=1)\n",
        "    tot_res['lower'] = tot_res.apply(lambda x: np.mean(x) - np.std(x), axis=1)\n",
        "    tot_res['two_upper'] = tot_res.apply(lambda x: np.mean(x) + 2*np.std(x), axis=1)\n",
        "    tot_res['two_lower'] = tot_res.apply(lambda x: np.mean(x) - 2*np.std(x), axis=1)\n",
        "\n",
        "    plt.plot(tot_res.mu, linewidth=4)\n",
        "    if prediction_dots:\n",
        "        plt.plot(tot_res.mu, 'bo', label='Likelihood mean')\n",
        "    plt.fill_between(x = tot_res.index, y1=tot_res.lower, y2=tot_res.upper, alpha=0.5)\n",
        "    plt.fill_between(x = tot_res.index, y1=tot_res.two_lower, y2=tot_res.two_upper, alpha=0.5)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "\n",
        "plot_uncertainty(ress, batch[1])"
      ],
      "metadata": {
        "id": "JMhIfbguZaPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate fit on training set\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "source_df['feature_1'] = source_df.feature_1.astype('float')\n",
        "X_batches = source_df.feature_1.values[:-3].reshape(-1, 20)\n",
        "y = source_df.target.values[:-3].reshape(-1, 20)\n",
        "\n",
        "predictions = []\n",
        "for batch in X_batches:\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_batch = scaler.fit_transform(batch.reshape(20, 1))\n",
        "    ress = []\n",
        "    for i in tqdm.tqdm(range(300)):\n",
        "        unscaled_prediction = get_sample_prediction(scaled_batch, dp_model.predict_theta_from_input)\n",
        "        ress.append(scaler.inverse_transform([unscaled_prediction])[0])\n",
        "    predictions.append(ress)\n",
        "\n",
        "# Concatenate batches and plot the whole time series\n",
        "prediction_concat = np.concatenate(predictions, axis=1, )\n",
        "ground_truth = np.concatenate(y, axis=0)\n",
        "plot_uncertainty(ress = prediction_concat, ground_truth=ground_truth,\n",
        "                 n_steps=140, figsize=(15, 9), prediction_dots=False)"
      ],
      "metadata": {
        "id": "JOiXPWIrZZZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xM51ewvhZthv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}